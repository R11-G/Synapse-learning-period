{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV Week 3 part 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVdR1p0yN1YE"
      },
      "source": [
        "# Class Models, Custom Layers, GPU training and Custom Resnets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jVXoMVAzLdj"
      },
      "source": [
        "## Instructions\n",
        "1. Use Google Collab for this task\n",
        "2. change runtime type to GPU in the Runtime tab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0OSmQlRtyMh"
      },
      "source": [
        "## Making models using class\n",
        "\n",
        "1. In earlier tasks, we learnt how to make a model using the sequential api provided by tensorflow.\n",
        "2. Now we learnt how to make a model using a class based approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T67gFCm5yKJ0"
      },
      "source": [
        "1. <b>Import</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvd3N1aMte77"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential, layers, initializers, models, callbacks, optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaaIDs0Xyeve"
      },
      "source": [
        "2. <b>To avoid GPU errors</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEZk_IRuydWT"
      },
      "source": [
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qszmpUatzwPs"
      },
      "source": [
        "3. <b>Loading MNIST Data</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw4x8BFxzHHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9aa98e5-f746-4d9d-c358-4603b5bdb96e"
      },
      "source": [
        "(x_train, y_train) , (x_test, y_test) = mnist.load_data()\n",
        "#flattening the images into 1 dimensional array\n",
        "x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CTHTVKj0jh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e80d497-e037-460b-8463-678d96528f80"
      },
      "source": [
        "len(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "849L-lA80lN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4381529c-3fb3-421b-b731-34bba28ef20a"
      },
      "source": [
        "len(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWRIJkRa05n6"
      },
      "source": [
        "4. <b>Building the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJARi9jQ0rfr"
      },
      "source": [
        "#we will extend the class from keras.Model\n",
        "#this means that we can use the functionalities of keras.model\n",
        "#extending a class is an object oriented concept, so google that if you are confused\n",
        "class MyModel(keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__() #super is also an object oriented thing which allows us to call the init method of the parent class (keras.Model here)\n",
        "    self.dense1 = layers.Dense(64) #self links the variable to the class. You can access dense1 anywhere in this class by using self.dense1\n",
        "    self.dense2 = layers.Dense(10)\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    x = tf.nn.relu(self.dense1(input_tensor))\n",
        "    x = self.dense2(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoTVfSbf6l_Q"
      },
      "source": [
        "5. <b>Creating the model object</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQx1h46h6lBR"
      },
      "source": [
        "model = MyModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6kOrpUT6yRV"
      },
      "source": [
        "6. <b>Compiling the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-8eI1MO6tYS"
      },
      "source": [
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    metrics = ['accuracy'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DixE2_s473n-"
      },
      "source": [
        "7. <b>Fitting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwOXXNQS7La1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f85120-2c0a-4492-81b3-13602edbb91f"
      },
      "source": [
        "model.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3104 - accuracy: 0.9120\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1479 - accuracy: 0.9573\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8040308a50>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AO3QvJr8_VX"
      },
      "source": [
        "6. <b>Evaluate the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MLpsRt58GC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be956af-5d65-47b9-fb55-05d4eec921f8"
      },
      "source": [
        "model.evaluate(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1229 - accuracy: 0.9641\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.12287259846925735, 0.9641000032424927]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Fal70p9QZX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8GUhKhs-RIL"
      },
      "source": [
        "<b>Why class based</b>\n",
        "1. allows for better code visualization\n",
        "2. better connectivity\n",
        "3. helps in modularizing the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTQn7xVS-baA"
      },
      "source": [
        "<b>Now we'll learn to make the layers (like Dense) using classes</b><br><br>\n",
        "<b>Why do we need to dive till making layers?</b>\n",
        "1. sometimes when we make our own models, we want to do our operations.<br><br>\n",
        "  a. dense follows the operation WX+b<br><br>\n",
        "  b. while back propogation, tf uses it's own automatic derivative algorithm to calculate the derivative of this operation and uses it for gradient descent.<br><br>\n",
        "  c. What if we want to implement a dense operation which does (WX x 5) + (7 x b)? How will tensorflow account for this derivative?<br><br>\n",
        "  d. The answer is : by using the keras.Layer class to build the layer <br><br>\n",
        "\n",
        "2. Hence, we can do any complex operation using the Layer class and tensorflow will account for its derivative during back propogation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d00WbYM_Dgun"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jjbIKZ79fCX"
      },
      "source": [
        "## Making layers using classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LSso33-Doc8"
      },
      "source": [
        "1. <b>Building the Dense layer from scratch</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS88cD5C9mbX"
      },
      "source": [
        "class MyDense(layers.Layer):\n",
        "  #when object created, first init is called\n",
        "  def __init__(self, units=32):\n",
        "      super(MyDense, self).__init__()\n",
        "      self.units = units\n",
        "\n",
        "  #init automatically calls build and the weights are set. init passes the input shape to build automatically. (input shape for dense 1 will be (batch_size, 784) 28*28=784, for dense 2 -> (batch_size,64))\n",
        "  def build(self, input_shape):\n",
        "      print(input_shape)\n",
        "      self.w = self.add_weight(shape=(input_shape[-1], self.units), #why do we choose this shape? ponder (make a neural net diagram and see what shape fits the matrix multiplication W*X)\n",
        "                               initializer='random_normal', #what other initialization methods exist?\n",
        "                               trainable=True)\n",
        "      self.b = self.add_weight(shape=(self.units,),\n",
        "                               initializer='zeros',\n",
        "                               trainable=True)\n",
        "\n",
        "  #build calls the call function which executes the operation\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOweaJ9gFcwG"
      },
      "source": [
        "2. <b>Using custom Dense to build MyModel</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1VtughXFbrZ"
      },
      "source": [
        "#we will extend the class from keras.Model\n",
        "#this means that we can use the functionalities of keras.model\n",
        "#extending a class is an object oriented concept, so google that if you are confused\n",
        "class MyModel(keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__() #super is also an object oriented thing which allows us to call the init method of the parent class (keras.Model here)\n",
        "    self.dense1 = MyDense(784) #self links the variable to the class. You can access dense1 anywhere in this class by using self.dense1\n",
        "    self.dense2 = MyDense(64)\n",
        "      \n",
        "  def call(self, input_tensor):\n",
        "    ###\n",
        "    #complete this code\n",
        "    #x = tf.nn.relu(self.dense1(input_tensor))\n",
        "    #x = self.dense1(x)\n",
        "    x = tf.nn.relu(self.dense1(input_tensor))\n",
        "    x = self.dense2(x)\n",
        "    return  x\n",
        "    ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9jXYFrmFxUZ"
      },
      "source": [
        "3. <b>Training the model </b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e0orLBUFvKP",
        "outputId": "24d13b1f-3c5b-4f3a-ca2e-2acb0363407c"
      },
      "source": [
        "#train the model\n",
        "#write code here\n",
        "model = MyModel()\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    metrics = ['accuracy'],\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "(32, 784)\n",
            "(32, 784)\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2015 - accuracy: 0.9409\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0786 - accuracy: 0.9760\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f80400a18d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j_Nuwg8KJmL"
      },
      "source": [
        "4. <b>Testing the model </b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu9nFqd4F5wP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39fed94-7984-40c7-e5b1-d514a72ff2a4"
      },
      "source": [
        "#evaluate the model\n",
        "#write code here\n",
        "model.evaluate(x_test, y_test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0748 - accuracy: 0.9761\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07479820400476456, 0.9761000275611877]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1P0__HUKSOJ"
      },
      "source": [
        "#### We got a very similar result with our own Dense Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzXSNjRuKPl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Saeq1rcLWNg"
      },
      "source": [
        "## Now your turn\n",
        "1. We will build a mini custom version of ResNet CNN architecture using the keras.Model\n",
        "\n",
        "2. Resnet solves the problem of vanishing gradients (https://www.youtube.com/watch?v=JIWXbzRXk1I) and allows us to build very deep neural networks.\n",
        "\n",
        "\n",
        "RESNET BLOG RESOURCE : https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8\n",
        "\n",
        "RESNET VIDEO RESOURCE : https://www.youtube.com/watch?v=sAzL4XMke80&t=314s\n",
        "\n",
        "Read the above blog carefully and watch the video for better understanding as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGrDssFOM-dm"
      },
      "source": [
        "#### <b>Our Custom Model architecture</b>\n",
        "DATASET -> MNIST\n",
        "\n",
        "1. Input -> 28,28,1 images\n",
        "2. 8 filters of 5x5, padding none => (24,24,8)\n",
        "3. Resblock 1\n",
        "  - 16 filters of 7x7, padding none => (18,18,16) <b>[im_res_1]</b>\n",
        "  - 16 filters of 8x8, padding same => (18,18,16)\n",
        "  - 16 filters of 10x10, padding same => (18,18,16) <b>[output_res_1]</b>\n",
        "  - (sum of output_res_1 and im_res_1 here) => (18,18,16)\n",
        "4. maxpooling => (9,9,16)\n",
        "5. Resblock 2\n",
        "  - 32 filters of 6x6, padding none => (4,4,32) <b>[im_res_2]</b>\n",
        "  - 32 filters of 7x7, padding same => (4,4,32)\n",
        "  - 32 filters of 8x8, padding same => (4,4,32) <b>[output_res_2]</b>\n",
        "  - (sum of output_res_2 and im_res_2 here) => (4,4,32)\n",
        "6. maxpooling => (2, 2, 32)\n",
        "7. Flatten => (128,1)\n",
        "8. Dense, 64 units => (64,1)\n",
        "9. Dense, 10 units => (10, 1)\n",
        "10. use softmax to predict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb1biy3uSRDd"
      },
      "source": [
        "### <b>Instructions</b>\n",
        "1. Build the custom Resnet model based on the architecture described above\n",
        "2. Only use class based approach\n",
        "3. Create a custom model using class based approach. Name the Class CustomResNet\n",
        "4. Use RELU as activation function, Batch normalization after every layer\n",
        "5. reload the data as it was already flattened before\n",
        "6. Use custom layers only if you find the use case for it here \n",
        "\n",
        "### <b>Notes</b>\n",
        "1. all layers are supposed to be defined in the init method.If defined anyehere else, will result in errors.\n",
        "2. Repititive layers will need seperate initializations\n",
        "3. you can check if your model is correct by the following code : -\n",
        " - model = CustomResNet()\n",
        " - model.build((None,28,28,1))\n",
        " - model.summary()\n",
        "4. Summary must have all layers defined and the trainable paramaetrs should not be zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYTxhj0pWmPO"
      },
      "source": [
        "#imports that you can use\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, BatchNormalization, Activation, Add, Input, MaxPool2D\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY2ag3fdas_m"
      },
      "source": [
        "# load the mnist dataset here\n",
        "# make sure to resize the dataset to consider the 3rd image dimension as well\n",
        "# dataset shape must be (num_images, height, width, channel)\n",
        "# divide data into train and test\n",
        "\n",
        "# write code here\n",
        "from keras.applications.resnet import ResNet\n",
        "from keras.applications.resnet import preprocess_input\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "x_train=x_train.reshape(-1,28,28,1)\n",
        "x_test=x_test.reshape(-1,28,28,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvxmpl1OU67p"
      },
      "source": [
        "#make a class based model for Custom ResNet (refer instructions and notes above)\n",
        "# write code here\n",
        "from tensorflow import keras\n",
        "class CustomResNet(keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CustomResNet, self).__init__()\n",
        "\n",
        "    self.conv1 = layers.Conv2D(filters=8,kernel_size=(5,5),input_shape=(24,24,1), strides=(1,1), padding = 'valid',activation ='relu')\n",
        "    self.batchnorm1 = layers.BatchNormalization()\n",
        "\n",
        "    self.conv2 = layers.Conv2D(filters=16,kernel_size=(7,7),strides = (1,1),activation='relu',padding='valid')\n",
        "    self.batchnorm2 = layers.BatchNormalization()\n",
        "    self.conv3 = layers.Conv2D(filters=16,kernel_size=(8,8),strides = (1,1),activation='relu',padding='same')\n",
        "    self.batchnorm3 = layers.BatchNormalization()\n",
        "    self.conv4 = layers.Conv2D(filters=16,kernel_size=(10,10),strides = (1,1),activation='relu',padding='same')\n",
        "    self.batchnorm4 = layers.BatchNormalization()\n",
        "\n",
        "    self.maxpool1 = layers.MaxPool2D(pool_size=(2,2),strides = None,padding='valid')\n",
        "    self.batchnorm5 = layers.BatchNormalization()\n",
        "\n",
        "    self.conv5 = layers.Conv2D(filters=32,kernel_size=(6,6),strides = (1,1),activation='relu',padding='valid')\n",
        "    self.batchnorm6 = layers.BatchNormalization()\n",
        "    self.conv6 = layers.Conv2D(filters=32,kernel_size=(7,7),strides = (1,1),activation='relu',padding='same')\n",
        "    self.batchnorm7 = layers.BatchNormalization()\n",
        "    self.conv7 = layers.Conv2D(filters=32,kernel_size=(8,8),strides = (1,1),activation='relu',padding='same')\n",
        "    self.batchnorm8 = layers.BatchNormalization()\n",
        "\n",
        "    self.maxpool2 = layers.MaxPool2D(pool_size=(2,2),strides = (1,1),padding='valid')\n",
        "    self.batchnorm9 = layers.BatchNormalization()\n",
        "\n",
        "    self.flat = layers.Flatten()\n",
        "    #self.batchnorm10 = layers.BatchNormalization()\n",
        "    self.dense1 = layers.Dense(64,activation='relu')\n",
        "    #self.batchnorm11 = layers.BatchNormalization()\n",
        "    self.dense2 = layers.Dense(10,activation='softmax')\n",
        "    \n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    x = tf.nn.relu(self.conv1(input_tensor))\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.conv2(x)\n",
        "    x1 = self.batchnorm2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.batchnorm3(x)\n",
        "    x = self.conv4(x)\n",
        "    x2 = self.batchnorm4(x)\n",
        "    x = x1 + x2\n",
        "    x = self.maxpool1(x)\n",
        "    x = self.batchnorm5(x)\n",
        "    x = self.conv5(x)\n",
        "    x3 = self.batchnorm6(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.batchnorm7(x)\n",
        "    x = self.conv7(x)\n",
        "    x4 = self.batchnorm8(x)\n",
        "    x = x3 + x4\n",
        "    x = self.maxpool2(x)\n",
        "    x = self.batchnorm9(x)\n",
        "    x = self.flat(x)\n",
        "    #x = self.batchnorm10(x)\n",
        "    x = self.dense1(x)\n",
        "    #x = self.batchnorm11(x)\n",
        "    x = self.dense2(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRWTnXtxaEEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c030cc8-b2c4-4885-f200-f58749fdb0ba"
      },
      "source": [
        "#find summary of model\n",
        "# write code here\n",
        "model_1 = CustomResNet()\n",
        "model_1.build((None,28,28,1))\n",
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"custom_res_net_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_14 (Conv2D)          multiple                  208       \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  multiple                 32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          multiple                  6288      \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  multiple                 64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          multiple                  16400     \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  multiple                 64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          multiple                  25616     \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  multiple                 64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  multiple                 64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          multiple                  18464     \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  multiple                 128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          multiple                  50208     \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  multiple                 128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          multiple                  65568     \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  multiple                 128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  multiple                 0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  multiple                 128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         multiple                  0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             multiple                  18496     \n",
            "                                                                 \n",
            " dense_7 (Dense)             multiple                  650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 202,698\n",
            "Trainable params: 202,298\n",
            "Non-trainable params: 400\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAo3M5HaaJ8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "0b3c8f98-a786-4c4e-b939-27f237ad7aaa"
      },
      "source": [
        "#train model here\n",
        "\n",
        "model_1.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    metrics = ['accuracy'],\n",
        ")\n",
        "model_1.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:4527: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6071b69a6a4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-f55536f09dff>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"conv2d_14\" (type Conv2D).\n\nValue for attr 'T' of uint8 is not in the list of allowed values: half, bfloat16, float, double, int32\n\t; NodeDef: {{node Conv2D}}; Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\", \"EXPLICIT\"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]> [Op:Conv2D]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(32, 28, 28, 1), dtype=uint8)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvo7QLJNJLGA"
      },
      "source": [
        "#evaluate model here\n",
        "model_1.evaluate(x_test,y_test,batch_size=32,epoch=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzKdD3XAMrir"
      },
      "source": [
        "## Congratulations!\n",
        "#### Now you know how to build class based models and custom layers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcEpZTjsKdvM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}